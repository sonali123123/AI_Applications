 If you're interested in AI development, then you're in the right place. Today, I'm going to be showing you how to build an advanced AI agent that uses multiple LLMs. We're going to run our agent through a series of steps. We're going to connect the two agents together. We're going to parse the output and you're really going to see how to get a bit more advanced here and how to do some really cool stuff with LLMs running on your own computer. We're going to be doing everything locally. We're going to use something known as Olamma and LLama Index and you're really going to get a taste of what AI development is capable of in the short video that even beginner or intermediate programmers can follow along with. So with that said, let's get into a quick demo. Then I'll walk you through a step-by-step tutorial on how to build this project. So I'm here inside of VS Code and I'm going to give you a demo of how this works. Now, the concept is that we're going to provide some data to the model. This data will be a coding project that we've worked on. We've kept it simple for this video, but if you scaled this up, you could provide it a lot more code files and it can handle all of those potentially at the same time. So you can see that we have a readme.pdf file in our data directory and this is a simple kind of documentation for an API that we've written. We then have a test.py file and this is the implementation of that API in Python. So the idea here is we want our agent to be able to read in this information and then generate some code based on the existing code that we already have. So I've just run the agent here and I've given this a sample prompt that says read the contents of test.py and write me a simple unit test in Python for the API. Now the idea here is that we've provided this agent some different tools that can utilize and it will decide when it needs to utilize the tools and use them to get the correct information. So in this case, it will use a tool to read in the test.py file. It will then use a tool to actually parse and get access to the information in the readme.pdf. It's then going to generate some code for us and then what we'll do is use another model to parse that output and save it into a file. So you can see that when I ran this, it did exactly that. Now it doesn't always give us the best result because we are running these models locally and I don't have a super computer here. So I can't run the best possible models, but it's showing you what's possible and obviously all of this is free. You don't need to pay for anything because we're running it locally using all open source models. So what this did here is generate this test API file. You can see there's a few minor mistakes, but if I fix these up by just adding the correct parentheses and removing the escape characters here, you see that we actually have a functioning unit test written for our Flask API. So this is pretty cool. We might need to change this a little bit to make it work, but it just outputted all of that for us based on the context of the files that we provided to it. Now if we go here and read through kind of the output we're getting from the model, you can see that it's actually sharing with us its thought process. So it says the current language of the user's English. I need to use this tool to help me answer the question. It's using the code reader tool. It passes the file name equal to test.py. It reads in the contents of test.py and then it says, okay, I can answer without using any more tools. I'll use the user's language to answer. To write a simple unit test below a blog, you do this. It then writes all of this code. Now behind the scenes, what actually happens is we parse the output of this using a secondary model and take just the code and then generate it into a file. And that file name will be generated by a different LLM to make sure it's appropriate for the type of code that we have. So it is a multi-step process here. It worked quite well. And you can see it also gave us a description of what the finished code was. So that's what I'm going to be showing you how to build. I know that it might seem simple, but it's actually fairly complex and it uses a lot of different tools which are really interesting to learn about. With that said, let's get into the full tutorial here and I'll explain to you how we can build this out completely from scratch. So let's get started by understanding the tools and technologies that we need to use to actually build out this project. Now what we need to do for this project is we need to load some data. We need to pass that to our LLM. We then need to take the result of one LLM, pass it to another LLM, and then we need to actually use a tool and save this to a file. There's a few different steps here. And if we wanted to build this out completely from scratch, that would take us a very long time. So instead, we're going to use a framework. Now of course, we're using Python, but we're also going to use LOM index. Now I've teamed up with them for this video, but don't worry, they are completely free and they provide an open source framework that can handle a lot of this heavy lifting and specifically is really good at loading in data and passing it to our different LLMs. I'm on their website right now just because it explains it nicely, but you can see LOM index is the leading data framework for building LLM applications. It allows us to load in the data, which you'll see right here, index the data, query it, and then evaluate it and also gives us a bunch of tools to connect different LLMs together to parse outputs. You're going to see a bunch of advanced features in this video. Now as well as using LOM index, we're going to use something called OLAMA. Now OLAMA allows us to run open source LLMs locally on our computer. That means we don't need to pay for chat GPT, we don't have to have an open AI API key, we can do all of this locally. So the summary here is that we're using Python, LOM index, OLAMA, we're also going to throw in another tool which is new from LOM index called LOM apart, so don't worry, it's free as well. And all of that is going to allow us to build out this advanced AI agent that has rag capabilities, rag meaning retrieval augmented generation, whenever we're taking this extra information and passing it into the model, that's really known as rag. I have an entire video that discusses how rag works, you can check that out here, but for now let's get into the tutorial and let's see exactly how we can build this application. So I'm back on my computer and we're going to start by installing all the different dependencies that we need, we're then going to set up OLAMA and then we'll start writing all of this code. Again, OLAMA is how we run the models locally, we need to install that first before we can start utilizing it. Now there are some prerequisites here, so what we're going to do is I have a GitHub repository that I'll link in the description and in that GitHub repository you'll find a data directory that contains a readme file and a test.py file. Now you don't need to use these specific pieces of code, but what you should do is create a data directory in a directory in VS code, so I've just opened up a new one here in VS code, made a new folder called data and then put these two files inside of here, specifically we want some kind of PDF file and some kind of Python file, we can have multiple of them if you want, but the concept is this is the data that we're going to use for Ragn, the retrieval augmented generation, so you need something inside of here, so either take it from the GitHub repository or populate it with your own data. Now from the GitHub repository as well, there is a requirements.txt file, please copy the contents of that file and paste it into a requirements.txt file in your directory or simply download that file. This is just going to save you a lot of headache because it has all of the specific versions of Python libraries that we need in order for this project to function properly. So really again, we want to get this data directory populated with some kind of PDF and some kind of Python file, we then want this requirements.txt file, again you can find it from the link in the description, I'll put a direct link to the requirements.txt, so you can just copy the contents of the file or you can download the file directory directly, sorry, and put it inside of your VS code folder. Now that we have that, what we're going to do is make a new Python virtual environment, that's where we're going to install all these different dependencies that we have this isolated on our system. So to do that, we're going to type the command Python 3-mv and then I'm going to go with the name of AI, you can name this anything that you want. If you're on Mac or Linux, this is the correct command. If you're on Windows, you can change this to simply be Python and this should make a new virtual environment for you in the current directory. This is where we'll install all of the different Python dependencies. Now once we've done that, we need to activate the virtual environment, the command will be different depending on your operating system, if you are on Windows or sorry, if you're on Mac or Linux, so Mac or Linux here, you can type source, the name of your virtual environment, which in this case is AI, slash bin, and then slash activate, and you'll know this is activated if you see the AI prefix. You can ignore this base prefix for me, it's just because I have kind of a separate installation of Python, but you should see this prefix in your terminal indicating that this is activated. Now if you're on Windows, what you're going to do is open up PowerShell and you should be able to type slash AI slash and then this I believe is scripts and then slash activate and that should activate the virtual environment for you. Otherwise, you can just look up how to activate a virtual environment on Windows and again, make sure you have this prefix. Once the virtual environment is activated, if we want to deactivate it, we can type deactivate, we're not going to do that though, and we can install the different packages that we need. So what we can do is type pip3 install dash r and then requirements dot txt. Notice this is in the current directory where we are. When I do that, it's going to read through all of the different requirements and then install them in this Python installation or in this virtual environment. So we'll do that. It's going to read through requirements dot txt and install everything for you. This is going to take a second for me. It's already cashed. So it's going pretty quickly, and that should be it. So once this is finished, I'll be right back and then we can move on to the next steps. All right. So all of that has been installed and the next thing we need to do is install ola. Ola again, let's us run all of this locally. So in order to install Ola. I'm just going to clear in my terminal here, and I'm going to go to a new browser window and paste in this URL here. I'm going to leave it in the description. Now, this is the GitHub page for Ola. And it shows you the installation setup steps here. So again, this will be linked in the description. So if you're on Mac or Windows, you can see the download buttons right here. Linux, this will be the command. I'm on Mac. So I'll just click on download. When I do that, it's going to download the Ola. I'm a installation for me. Once that's done, I'm going to unzip this and then I'm going to run the installer. Now, I've already installed it, but I will still run you through the steps. And then on Windows, same thing, you're going to download this and then run through the installer. And what this will do is download a kind of terminal tool for you that you'll be able to utilize to interact with Ola. As you can see, it says Ola run and then something like Lama 2. And this will actually download the Lama 2 model for you and then allow you to utilize it and interact with it. In our case, we're actually going to use the Mistro model, but there's a bunch of different models here that you could install. And there's a bunch of other ones as well. These are just some examples of ones that you can use. Okay, so what we'll do here is unzip this folder. And once it's unzipped, we're going to run the installer. So you can see here that I can click on Ola. That's going to load the installation tool for me. So I'm going to go ahead and click on open. We're going to move it into applications and then we should be good to go with Ola. So we'll go next and then it says install the command line. Okay, so we're going to go ahead and install it. Again, I already have this installed. So I'm not going to run through this tool. But once you do that, you should be able to run the Ola command, which will do in just one second. All right, so once you've gone through that installer, what you can do is simply open up a terminal, which we're going to do here. And we can type, let me just zoom in here so we can read this Ola. And just make sure that that command works. So if we get some kind of output here, we're good. And then we can type Ola. Run and then we're going to run at the Mistral model. Okay, so you can see here, it shows you all the different models you can potentially run. In this case, this one is seven billion parameters. It's 4.1 gigabytes. There's a lot larger models here, which obviously would perform better, but they need some more intense hardware to actually run properly. So we're going to install Mistral, which is only four gigabytes by doing Ola. Run Mistral. It's going to then download that model for you. And then we can utilize it. Now, in my case, it's already downloaded. So what I can do is start interacting with it by typing something like hello world. And then it's going to give me some kind of output. Perfect. So what I'm going to do now is I'm going to quit this. So I think I can just type quit or something or control C control D. Okay, let's get out of that control D. I'm going to close this terminal. And I'm going to show you now how we can run this from code. So let it go through, let it install. It is going to take a second because that's the download all of this stuff. And then we'll go back to VS code and see how we interact with Ola from our code. All right. So I'm back inside of VS code here. And I'm going to continue here by creating a file. Now this file will be main.py. And the idea here is just to initially test Ola and make sure that it's working as an LLL. So I'm going to say from, and this is going to be Lama underscore index dot LLM's if we type this correctly dot O llama like that. We're going to import O llama. And then we're going to say LLM is equal to O llama. And inside of here we're going to say the model is equal to mistral because this is the one that we want to use. And we can provide a request timeout equal to something like 30 seconds just so that it doesn't take too long. Now that we have the LLM we should be able to do LLM dot run. And then we can say something like hello world. If we say the result is equal to this we should be able to print out the result. So let's see if that's going to work. I can type Python 3 and then main dot pie. We'll give this a second and we'll see if that was valid command or not or if we need to use a different one. So actually my bad here guys rather than LLM dot run this is going to be LLM dot complete. And then we can type in something like hello world. And if we run this we should see that we get some kind of output. Give this a second. It says hello. Here's a simple hello world program. Give this the output. And there we go. So this was just a simple test to make sure that O llama was running locally on our computer. We also can run different types of local models. For example, in a second we're going to run a code generation one. But now you can see that this is indeed working and we didn't need to have any API key use chat GPT etc. This is a local model running on our own computer. So now what we want to do is set up a little bit of the rag application so we can load in some files. Pass that to the LLM and see how it can query based on that. All right. So let's go to the top of our program here. And we're going to import a few things that we need in order to load our Python file as well as to load our documentation. We're going to start by looking at how we load in our PDF, which is unstructured or semi structured data, depending on the way that it's set up. And we're going to use something known as llama parse, which can give us a much better parsing of this file. So what I'm going to do is say from llama underscore parse, we are going to import and then with capital llama parse like that. We'll talk about this in one second. Don't worry. I'm going to say from llama underscore index dot core. And we're going to import the vector store index and the simple directory reader as well as the prompt template while we are here. We're then going to say from llama underscore index dot core dot. And this is going to be embeddings. And we are going to import the resolve embed model. And I believe for now that is actually all that we need. So let me break down what we're about to do here. We need to load in our data. Now in this case, we're loading in PDF documents, but with llama index, we could load in really any type of data we want. And in the previous video, I showed you how to load in, for example, CSV data. But in this case, we have a PDF. Now, what we need to do is we need to parse the PDF into logical portions and chunks. For example, if the PDF had something like a chart, we'd want to extract that because that's some structured data that we could be able to look at. Then once we have that, we need to create a vector store index. Now a vector store index is like a database that allows us to really quickly find the information that we're looking for rather than having to load the entire PDF at once. What's going to happen is our LLM is going to utilize this database and extract just the information that it needs to answer a specific query or a prompt. Now the way that will build this vector store index is by creating something known as vector embeddings vector embeddings take our textual data or whatever type of data it is. And they embed it into multi-dimensional space, which allows us to query for it based on all different types of factors based on the context, based on the sentiment. We don't really know exactly how it works. It's handled by LLM since some machine learning models in the background. And I'm not quite qualified to talk about it in this short section of the video. But the point is that rather than loading all of the data at once, we're going to query this vector store index, which is like a really, really fast database. It's going to give us the information we need, inject it into the LLM and then the LLM will use that information to answer the prompt. So really all that means for us is we've got to create this index and I'm going to show you how to do that. All right, so to do this, we're going to delete these two lines because these were really just for testing, but we will leave this LLM. And what we're going to do is start by setting up a parser. Now the parser is going to be a llama parse and then we can specify what we want the result type to be, which in this case is marked out. Now llama parse is a relatively new product that's provided by llama index. What this will do is actually take our documents and push them out to the cloud. They'll then be parsed and then that parsing will be returned to us. Now the reason we use something like this is because it gives us significantly better results when we are trying to query pieces of data from something like a PDF, which is typically unstructured. I'll talk more about it in a second because we do need to make an account with llama parse. But again, it's totally free. You don't need to pay for it. So we're going to make this parser. And then what we're going to do is we're going to create a file extractor. Now the extractor is going to be a dictionary. And we're going to specify a file extension, which in this case is .pdf. And we're going to say whenever we find a PDF, we want to use this parser, which is llama parse, to parse through the PDF and then give us back some results that we can then load. Next, we're going to say documents is equal to, and this is going to be the simple directory reader. And inside of here, we're going to specify the directory that we want to read from, which is the data directory. And then we're going to specify our file extractor here. So file extractor is equal to the file extractor that we specified. And then we're going to say .load data. Okay, so let's write that. Now if we hover over this, you can see that what this is doing is loading data from the input directory. So we're using llama index. We have something called a simple directory reader. What this will do is go look in this directory, grab all of the files that we need, and then load them in and use the appropriate file extractor. Now that we've done that, what we can do is we can pass these different documents, which have been loaded to the vector store index and create some vector embeddings for them. So we're going to say the embed underscore model is equal to the resolve embed model. And then this is going to look a little funky, but we're going to type local colon and then BAAA, I slash BGE-M3. Now this is a local model that we can use because by default when we create a vector store index, it's going to use the open AI model, like something like chat GPT. We don't want to do that. We want to do this locally instead. So what we're doing is we're getting access to a local model. And this model will be able to create the different vector embeddings for us before we inject this data into the vector store index. So I know it seems a bit weird, but we're just grabbing that model. This is the name of it here, and we're specifying we want it locally, which means the first time we run this, it's going to download that model for us and then use it. Okay, we're then going to say the vector index is equal to the vector store index and then dot from documents. And then we're going to pass the documents that we've loaded here with the simple directory reader. And we're going to specify manually the embed model is equal to the embed model that we got above, which is our local embedding model. Now that we've done that, we're going to wrap this in something known as a query engine, so we can actually utilize it to get some results. So we're going to say query engine is equal to the vector index dot as query engine. And the LLM that we're going to use is going to be the Olamma LLM. Now what this means is that I can now utilize this vector index as kind of like a question and answer bot. So what I can do is I can ask it a question like what are the different routes that exist in the API, and it will then go utilize the documents that we've loaded in, which in this case are the PDF documents in this readme.pdf file, and it will give me results back based on that context. Now in order to test that, we can say query engine dot, and then we can actually send this a query, and we can say what are some of the routes in the API question mark, and then it should give us back some kind of reasonable response. Now we do need to print that. So we'll say result is equal to this, and we will be able to run this code in one second. Once we get access to the API key for LLMAParse, so let me show you how we do that. So I am going to show you how to use LLMAParse here, but I quickly want to break down what it actually is. So on February 20th, 2024 LLMAParse released LLMAParse and LLMAParse. Now this brings production grade context augmentation to your LLM and Ragn applications. LLMAParse specifically is a proprietary parsing for complex documents that contain embedded objects such as tables and figures. In the past, when you were to do some kind of querying over this data, you get really, really bad results when you have those embedded objects. So LLMAParse is kind of the solution to that where it will do some parsing and actually break out these embedded objects into something that can be easily ingested and understood by your model. This means you'll be able to answer complex questions that simply weren't possible previously. As you can see, Ragn is only as good as your data. If the data is not good, if the vector index isn't good, then we're not going to get good results. So the first step here is that we parse out our documents into something that's more effective to pass into the vector index. So when we eventually start using it, we get better results, which drastically affect the accuracy in a good way. So you can kind of read through here and you can see exactly what it does. I'll leave this link in the description, but just understand that what this does is give us much better results when we are parsing more complex documents, specifically things like PDFs. So what we're going to do here is create a new LLMAPCLAUD account. You can do that just by signing in with your GitHub. I'll leave the link below and this will give us access to a free API key so we can use the LLMAParse tool. All right. So once you've created that account or signed in, you can simply just click on use LLMAParse. It's pretty straightforward here. And then what you can do is you can use this with a normal API or you can use it directly with LLMAPCLAUD index, which is exactly what we're doing here. So what we want to do is just get access to an API key here so we can click on API key and we can generate a new key. I'm just going to call this tutorial. I'm going to press on create new key. You can read through the docs if you want, but I'm just going to copy this key. We're going to go back into our Python file and I'm going to make a new dot ENV file here. And then I'm going to create an environment variable that stores this key. I will have access to in our code. So we're going to say that this is llama underscore cloud, underscore API underscore key. This is going to be equal to and then I'm going to paste in that API key. Obviously, make sure you don't leak this. I'm just showing it to you for this video and I'll delete it afterwards. Then we're going to go into main.py or just going to load in that environment variable and it will automatically be detected by our parser. So I know I went through that quickly, but the basic idea is we're going to make a new account here on llama cloud. We're then just going to use the free parser. So we're going to go and generate an API key. Once we generate the API key, we're going to paste that inside of an environment variable file with the variable llama cloud API key. We're going to close this. Then we're going to go to our Python file and we're going to write the following code, which will allow us to automatically load in this environment variable. So we're going to say in lower cases from dot ENV imports load underscore dot ENV. We need to spell these correctly, though. And then we're going to call this function and what the load dot ENV function will do is look for the presence of a dot ENV file and then simply load in all of those variables, which will give this line right here, the parser access to that variable so we can use llama parse. Great. So now that we've ridden this code, we can test this out. So what I'm going to do is type Python three and then main dot pie. We're going to wait a second for it to install everything that we need. And then we're going to see if we get some kind of output. All right, so this is finished running and you'll see the what happened here is it started parsing this file using llama parse. It actually pushed that out to the cloud, which means if we had hundreds of files, thousands files, etc, we can actually handle all those push them out to llama parse and then get the results back. Then what it did is gave us the result here after querying that PDF. So it says the API supports several roots for performing various operations. These include slash items, slash items, items ID items ID, etc. So it used that context to actually answer the question for us. So now that we've created this, this is going to be one of the tools that we provide to our AI agent. The idea here is that we're going to have this and we're going to have a few other tools. We're going to give it to the agent and the agent can use this vector index and this query engine to get information about our PDF or about our API documentation and then using that information, it can generate a new response and answer questions for us. So the agent is going out there utilizing multiple different tools. Maybe it combines them together. Maybe it uses one, maybe it uses two, three, etc. And then aggregates all of the results there and gives us some kind of output. So now let's look at how we start building out the agent and we'll build out another tool that allows us to read in the Python file because right now we're just loading in the PDF. All right, so we're going to go back to the top of our program here and we're going to say from llama underscore index dot core dot tools and we are going to import the query engine tool and then the tool metadata. Then what we're going to do is we're going to take this query engine. So I'm going to delete this right here and we're going to wrap it in a tool that we can provide to an AI agent. So I'm going to say tools are equal to and then this is going to be query engine tool for the query engine. This is going to be the query engine for our PDF or for our API documentation. We're then going to say metadata is equal to and then this is going to be the tool metadata. And here we're going to give this a name and a description. Now the name and the description will tell our agent when to use this tool. So we want to be specific. So I'm going to call this API documentation and then we want to give this a description. So we're going to say the description is equal to and I'm just going to paste in the description to save us a little bit of typing here. Okay, so let's paste it in and this says this gives documentation about code for an API. Use this for reading docs for the API. Okay, so we're just giving some information about the query engine tool. Now we are going to write another tool in here in a second, but for now I want to make the agent and then show you how we utilize the agent. So we need to import another thing in order to use the agent here. So we're going to go up to the top and we're going to say from llama index dot and this is going to be core dot agent and we're going to import the react agent. Okay, we're now going to make an agent. So we're going to say agent is equal to react agent dot from underscore tools. And we're going to give it a list of tools that it can use. Okay, so we're going to say tools and then we need to give it an LLM which we're going to define in one second. We're going to say verbose equals true. If you do this, it will give us all of the output and kind of show us the thoughts of the agent. If you don't want to see that, you can make this false. And then we're going to provide some context to this, which for now will be empty, but we'll fill in in one second. Okay, so this is great. But what I want to do now is I want to make another LLM that we can use for this agent because we want this to generate some code for us rather than just be a general kind of question answer ball. So what I'm going to do here is I'm going to say my code LLM is equal to O llama. And we're going to use a different LLM. And this is going to be model equal to code llama. Now code llama is something that does code generation specifically. So rather than using the normal mistral model, which we had here, we're just going to use the code LLM because we want to do code generation. So now I'm going to pass code LLM here. And you can see how easy it is to utilize multiple LLMs locally on your own computer. Again, all of these are open source. And when you do this, it should automatically download it for you. Okay, last thing we want to do is provide a bit of context to this model. So just to clean up our code a bit, we're going to make a new file. It will call this prompts dot pi. And inside of prompts, I'm just going to paste in a prompt for the context for this model. Okay, you can find this from the link in the description from the GitHub, but it says purpose. The primary rule of this agent is to assist users by analyzing code. It should be able to generate code and answer questions about code provided. Now you can change that if you want, but that's really what it's doing. Right, it's going to read code, analyze it, and then generate some code for us. So that's what we're doing. So now what I want to do is import that context. So I'm going to go to the top. I'm going to say from prompts, imports, imports, and then context. And then down here, I'm going to pass that context variable. Okay, so now we have made an agent and we can actually test out the agent and see if it utilizes these tools. So let's do a simple while loop here. And let me just make this a bit bigger. So we can see it. We're going to say, well, prompt, colon equals to input. And we're going to say enter a prompt. And we're going to say q to quit. So if you type in q, then we're going to quit. And we're going to say, well, all of that does not equal q. Okay, so let's type this correctly. Then we are going to do the following, which is result equal to agent dot query. And then we're just going to pass in the prompt and then print the result. Okay, so you might be wondering what we just did. Well, we simply wrote an inline variable here using something known as the wall res operator in Python. This just means it's only defined in the while loop. And it will get redefined each time the while loop runs, just make things a little bit cleaner. And we say, okay, let's get some prompt when it's not equal to q, then we'll simply take the prompt, pass it to our agent. The agent will then utilize any tools needs to. And then it will print out the result. So let's test this out and see if it's working. So let's clear and let's run. And this time we're not just going to be using the API documentation vector index. We'll use an agent and it will decide when to utilize that tool. I know it seems a bit weird because we only have one tool right now, but imagine we had 20 tools, 30 tools, 100 tools, then the agent would pick between all of them and have the ability to do some really complex stuff. All right, so this is running now. I'm going to give it a prompt. I'm going to say something like send a post request to make a new item using the API in Python. Okay, let's see what this is going to give us here. And if this is going to work or not. Okay, sweet. So it looks like that works. If we go here, we can see that we get, I need to use a tool to help me answer this question, API documentation. It's looking for post items. Okay, the API documentation provides information on how to create an item using the post method. I can answer the question to create a new item, blah, blah, blah. And then it generates the response. And then it gives it to us here, right? To create a new item, we do this import requests, URL payload response. Okay, that actually looks good. And then come down here and says this will create a new item and then we can ask it another question or hit Q to quit. Perfect. So that is working. However, I want to add another tool to this agent that allows it to load in our Python files. So a lot of parts itself can't handle Python files. That's actually not what it's designed for. But what we'll do is we'll write a different tool that can just read in the contents of any code file that we want and then give that into the LLM. So this way, I can have access to the API documentation. And I can also have access to the code itself so it can read both of them. So let's start doing that. And the way we'll do that is by writing a new file here called code reader dot pie. So let's go inside of code reader and we're going to make a new tool that will then pass to our LLM. So we're going to say from llama underscore index dot core dot tools imports the function tool. Now this is really cool because what we can do is wrap any Python function as a tool that we can pass to the LLM. So any Python code that you'd want the model to be able to execute, it could do that. You just have to give it a description of the tool and it can actually call that Python function with the correct parameters. This to me is super cool and it really has a lot of potential and possibilities. Now I'm also going to import OS and then I'm going to define a function which will act as my tool. So I'm going to say the code reader function and we're going to take in a file name. Okay. Now what we're going to do is say path is equal to OS dot path dot join and we're going to join the data directory and the file name because we want to look just inside of data here perfect. And then we're going to try to open this. So we're going to say try we're going to say with open. Okay. And this is going to be the path. And then we're going to try to open this in read mode as f. Then we're going to say the content equals f dot read. And then we can simply return the file underscore content. And this will be the content. Okay. Then we're going to have our accept exception as e. We got to spell accept correctly. And then what we're going to do here instead is we're going to return some kind of error. And that error is going to be the string of e. And that's it. That's actually all that we need for this function. Now this is something that we can wrap in a tool and we can provide to the agent. It can then call this function and get back either the file content or the error that occurred. Okay. So we're going to say the code underscore reader is equal to the function tool. And this is going to be dot from underscore defaults. And then we're going to say f and standing for function is equal to the code reader funk. And then what we need to do similar to before is we need to give this a name and we need to give this a description. So the agent knows what to use or when to use this. So we're going to call this the code reader. And for the description, I'm just going to paste in a description like I had before. Let me see if I can move this on to separate lines. Okay. Let's just do it like this so that you guys can read it. Okay. So it says this tool can read the contents of code files and return the results. Use this when you need to read the contents of a file. Perfect. That looks good to me. Hopefully that's going to work for us. And now we can go to main.py and we can import the code reader tool. So we're going to say from code reader import code reader, which is our tool, our function tool. And now we can just simply pass that in our list of tools. So imagine, right? You could write any Python function you want. Just wrap it like I said, or I did here with the function tool and then just pass it in this list. And now all of the sudden, your agent has access to this and it can start manipulating things on your computer, interacting with Python functions. This really makes the possibilities of agents quite unlimited. And that's what I really like about this. Okay. So we have the code reader tool now and we also have the API documentation. So now our agents should work exactly as before. And we can simply read the contents of that file. So let's try running this and see what result we get. When we give it a prompt that asks it to say read that file. Okay. So start parsing the file. And then we'll write a prompt and we'll say something like read the contents of test.py and generate some code. Okay. So read the contents of test.py and give me the exact same code back. Now remember we're running some local models that don't have a ton of parameters and aren't the best ones. So we're not always going to get the best result. But I hope this is going to work or it should give us at least some kind of result. So you can see it says, okay, I need to use a tool. So it says we're going to use the tool code reader file name test.py and then it gets the contents of the file. And then what it says here is you provide a Python script that contains an in-memory database for simplicity, which implements a list called items. The script defines for mpoints one for creating new items, blah, blah, blah, and then gives us this whole result. So it didn't give us the code that we wanted, but it did actually give us a description of what was inside of that file, which to me says this is indeed working. And notice it only used this tool. It didn't use the other tool because it didn't need that for that specific prompt. Okay. So I think that's good. That means that it's working and we're able to utilize both the tools. Now the next thing that we need to do is we need to take any code this model is generating for us. And we need to actually write that into a file. Now this is where we're going to use another LLM. So what we want to do is we want to get the result that we just saw there. We wanted to determine if it's valid code and then we want to take that code and write it into a file. Now in order to do that, we need an LLM to analyze the result of this output. So what we're really going to do, right? So we're going to take this result. We're going to pass it to a different LLM and that LLM is going to have the responsibility of taking that result and formatting it into something that we can use to write the code into a file. Now I'm not sure if I'll be able to show this here because I think I cleared the console. Yeah, I did. But you would have seen before that it gives us some code output. But the code is mixed with like descriptions and other information that we don't want to write into the file. So the other LLM's job is going to be to parse that output into a format where we can take it and we can write it into the file. So let's start writing that. This is where it gets a little bit more complicated, but I also think it's where it gets quite cool. So we're going to go to the top of our program here. And we're going to start importing some things that can do some output parsing for us. So we're going to say from PIDantic imports the base model. We're then going to say from llamaindex.core.output. And I believe this is underscore parsers. We are going to import the PIDantic output parser. We're then going to say from llamaindex.core.querypipeline imports the query pipeline which allows us to kind of combine multiple steps in one. So now we're going to scroll all the way down and after we create our agent and our code LLM, we're going to start handling the output parsing. So we're going to make a class here. We're going to say class code output. And this is going to be a base model from PIDantic. Then what we're going to do is define the type of information that we want our output to be parsed into. Now this is super cool because we can use llamaindex and these output parsers to actually convert a result from an LLM into a PIDantic object. So we can specify the type that we want in the PIDantic object. And then llamaindex and another LLM can actually format the result to match this PIDantic object. Super cool. So I'm going to say code and this is going to be type string. I'm going to say description and I want this to be string as well. But we could make it other types. But in this case, we're just going to need strings. And then I'm going to say file name is a string. Okay, so I've just made a PIDantic object. This is just a class that we're going to use to do our formatting. And then we're going to write some things for our query. So we're going to say parser is equal to the PIDantic output parser. If I can write this here, and we're going to pass the code output, which is specifying we want to use this PIDantic output parser and get our result and pass it into this code output object. We're then going to have a JSON prompt underscore string. And this is going to be equal to a parser dot format. And we're going to format the code parser template, which is a variable that I'm going to write in one second. So now what we're going to do is go to prompts. And I'm going to write in a prompt here. I'll explain how this works. We just got to bear with me because there is a bit of code that we need to write. Okay, so let me copy this in. Again, you can find this from the GitHub or you can just write it out yourself. And what this says is parse the response from a previous LLM into a description and a string of valid code. I also come up with a valid file name. This could be saved. I also come up with a valid file name. This could be saved as that doesn't contain special characters. Here is the response. And then this is the response from the previous LLM. You should parse this into the following JSON format. Okay, so this seems weird. But this is what I'm providing to my output parser to tell it how to take the result from this LLM and parse it into the format that I want. So let's import that. And then we'll look at how this works. So from here, we're going to do the code parser template. And then I'm going to pass the code parser template here. Now what the parser dot format will do is it will take this string and it will then inject at the end of that string the format from this pedantic model. So I've ridden my pedantic output parser past the code output. It's saying, hey, this is the format we want code string description string file name string. What the output parser will do when I do parser dot format is it will take this format find the JSON representation of it and then pass it or inject it into the code parser template. So then when I start using this template on the next step, it knows the type of format we want the output to be in. So now I say my JSON underscore prompt underscore template is equal to and this is a prompt template. And I simply pass my JSON prompt string. Now at this stage, what we do is we write a kind of wrapper on the prompt template so we can actually inject inside of here the response. So the response if we look here is this. Okay, just bear with me. This will make sense as we get there. And then lastly, we're going to make an output pipeline and the pipeline is going to look like this. It's a query pipeline and the query pipeline is going to have a chain and the chain is going to go that we first need to get the JSON prompt template. We're then going to get whatever we need in the template and then we're going to pass that to our LLM and notice this time I'm using my normal mistro LLM, which is this one right here. I'm not using the code LLM because I want a different LLM for this task, a more general purpose one, not one specifically for code. Okay, so now we have our output pipeline. So the idea here is that what we want to do is we want to take the output pipeline and we want to pass this result to it and then we're going to get the result back, which is going to be that formatted object that we want to look at. So I know this is a bit complicated, but that was kind of the point of this video was to make it a bit more advanced and you're going to see now how we do this. So we have the result from agent.query prompt. Now let's take that result and pass it to our next agent. So we're going to say next result is equal to the output pipeline dot run and we're going to pass the response equal to the result. So whatever the result was from the first agent, that's now what we're passing is this variable right here in this code parser template prompt. Then we can print out the next result and we can see what we get. Now keep in mind this doesn't always work. There is sometimes some errors based on how the parsing occurs, but overall it's pretty good. So let's go up here and let's run our agent again and let's get it to do a similar thing that I did before where it calls like a post endpoint or something. Okay, so a similar prompt as before read the contents of test.py and write a Python script that calls the post endpoint to make a new item. Let's type enter in here and let's see what we get. All right, so we can see that we get the kind of thought process here of the first LLM, which is that it needs to use the code reader tool, which it does and then it generates this code. And then what happens is we actually get output here from our second LLM that says assistant and then it gives us this Python object or this JSON object really where we have the code, which is all of the code that it generated, which was this right. And then it has the what else description user request library and Python to do this. And then it has a file name, which is this. So now that we have this kind of output, what we want to do is take this output and load it as kind of valid JSON, what do you call it data in Python, I'm going to show you a fancy way to do that. Once we have that, we can then access all the different fields like code, description, and file name and we can utilize those to save a new file on our computer. So again, we've gone through, we've generated the code, we've used our different tools from the rag pipeline. And then we've now parsed our output into this format, where we're able to utilize these different fields. We just now need to load this in so that it's kind of valid for us to be able to view. Okay, so now that we have that, what we're going to do is the following, we're going to say our cleaned JSON is equal to, and we're going to go up to the top far program, and we're going to import a st. Now, a st is something that going to allow us to actually load in Python code. So what we'll do is we'll take the output from here and we'll load it in as a Python dictionary. So we're going to say ast.literal evaluation, and then we're going to convert the next result into a string because it's actually a response object, and we're going to replace the assistant, which was kind of what was leading here with an empty string. So all we're doing is removing that assistant that came before that valid Python dictionary, and then we're loading in the rest of this as a Python dictionary object. So now this is actually going to give us a Python dictionary, and what I can do is I can print code generated, and then I can print what it is. So I can say my cleaned JSON and then access the code, and then I can print my description. So I'm going to go here and go backslash n, backslash n, description, and this needs to be a backslash, not a forward slash. Okay. And then the description will be the cleaned JSON of the description. I can then say my file name is equal to the cleaned JSON, and this will be my file name. Okay. So let's run this now and see if we get the correct output. And then we are just going to add some error handling here, and we're actually going to save the file because it is possible that some errors could occur. So let's save and let's bring this up and let's quit. And let's copy this prompt because this one worked, and we will paste it again, and we'll see if we're able to actually get all of those different fields. And if we load in the Python object properly. All right. So you can see here that we're getting our result code generated, which is this create item, and then it has some lambda function here. And then we have the description, and then we didn't print out the file name, but we would have had the file name as well. So it gave us a different result that we had last time. You can see this is indeed working. And now we can quit, and we can move to the next step, which is a little bit of error handling, and then actually saving the file. So what we want to do now is just make sure that this works before we move forward, because it's possible that we could get an error. So what we're going to do is retry this prompt or this sequence of steps a few times to just make sure it's hand or it's working properly. Sorry, before we move on to the next step. So we're going to say retries are equal to zero, and we're going to say, wow, retries are less than three. So we'll just retry this three times. Then inside of here, we're going to do this, and we're going to say try, and we're going to try the following. We're then going to say accept, and this is going to be exception as e. We're going to say retries plus equals one here, and then we're going to break if this happens successfully. So what's going to happen now is we're going to retry this up to three times. So every time we fail, something happens here that's wrong, we simply retry it, and it will go and do this again with the same prompt that we typed. Now we can also do an error message here, and we can say print, error, occurred, retry, number, and then we can make this an f string, and we can put in the number of retries, and then we can print out what the error actually was. Okay, so that should be our retry block. I'm now going to come down here, and I'm going to say, okay, if retries is greater than or equal to three, which means this block actually failed. We never successfully generated this cleaned JSON, then I'm simply going to say continue, which means we're going to go back up here and ask for another prompt, and I'm going to say print, unable to process request, try again. Okay, now you've probably seen this sequence before, but pretty much we'll try to do this. If it doesn't work, we'll just say, hey, you know, that prompt didn't work for some reason. Okay, give us another one, because it's possible they ask us to do something we're not able to do, or the outputs not possible. There's all kinds of errors that could occur here, so we're just kind of handling that and cleaning it up a bit with this logic. Now, if we do get down to this point here, that means that we were able to actually generate this code. So what we can do is we can save it to a file. So we can write a little try here, we can say try with open, and we can say open the file name, which we have here, and then we can say that we want to open this in W as f, and we can say f dot right, and we can write the clean JSON code into that file. Okay, then we can say print saved file, and we can just print out the file name, and then we can have an accept here, and we can say print error saving file. Okay, now just to make sure that we're not going to override a file name that we already have, what we can do is we can do an OS dot path dot join, and we can make an output folder here. So we can say output, and then file name. Now we need to import OS, so we'll go to the top of our program, and import OS. Sorry, I know I'm jumping around all over the place here, then we can go here and we can make a new folder called output. So now all of the output will just go inside of this folder, so we don't accidentally override any files that we already have. Okay, so kind of final run here, let's give this a shot and see if this works. So let's bring up the code, let's clear, and let's run, and then we'll enter our prompt, and we'll see if it gives us that generated code. Okay, so we're going to use the same prompt as before, and now what we're looking for is that we actually get a generated file inside of this output directory. Okay, so it seemed this did actually work. You can see it has the code generated here, and then if we go into our output, we have this create item file. Now we do need to remove this because there was a few characters that I guess it left in here, but what it's doing is looking for an access token, open up test.py, okay, f.read, response, request.post, access token, response.status. So it's not perfect. There's a few things that it probably shouldn't be doing here, but overall it gave us kind of some good starting code or at least kind of prove the point that hey, we can generate some code. It is attempting to call the API. It is calling it in the correct way. So yeah, I mean, I would call that a success. Obviously, we can mess around with a bunch of different prompts. We can see what once it works for, once it doesn't work for. Remember, we're using these local models, which are quite small, which don't have the same capabilities of something like Chatchy BT. If we did this at an enterprise level with the best hardware, with the best models, obviously we'd get some better results. But for now, I'm going to quit out of that model and I'm going to wrap up the video here. All of this code will be available to download from the link in the description. A massive thank you to Lama index for sponsoring this video. I love working with them. Their framework is incredible. And it really just opens my imagination and eyes to what's possible with these LLMs. I mean, look what we were able to create in about 30 or 45 minutes. Obviously, I was walking through it step by step. I was going slower than I would normally code this out. And we have an advanced AI agent that can do some code outputting and parsing. We're using multiple different agents locally. And we could continue to chain these and do some really cool things. Anyways, if you guys want to see more videos like this, definitely leave a comment down below. Like the video, subscribe to the channel. And I will see you in the next one.